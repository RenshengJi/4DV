# ICP Supervision Training Configuration

# =============== Data Configuration ===============
data_dir: './icp_supervision_data_real'  # Directory containing .npz sample pairs
train_ratio: 1.0                          # Train/val split ratio (set to 1.0 to use all data for training and skip validation)
seed: 42                                  # Random seed
cache_in_memory: false                    # Cache all data in memory (faster but uses more RAM)

# =============== Model Configuration ===============
# Gaussian Refine Network (Sparse Convolution based)
input_gaussian_dim: 14
output_gaussian_dim: 14
gaussian_feature_dim: 384            # Feature dimension (Plan C: 384 for higher capacity)
gaussian_num_conv_layers: 10         # Number of conv layers (Plan C: 10 layers)
gaussian_voxel_size: 0.05            # Voxel size in meters

# =============== Loss Configuration ===============
# Position loss is the most important for ICP supervision
position_weight: 10.0                # Weight for position (xyz) loss
scale_weight: 1.0                    # Weight for scale loss
rotation_weight: 1.0                 # Weight for rotation loss
color_weight: 1.0                    # Weight for color loss
opacity_weight: 1.0                  # Weight for opacity loss

use_smooth_l1: false                 # Use SmoothL1Loss instead of MSELoss
position_only: false                 # Only supervise position (ignore other params)

# Optional Chamfer loss (computed in point cloud space)
use_chamfer_loss: false              # Add Chamfer distance loss
chamfer_weight: 1.0                  # Weight for Chamfer loss

# =============== Training Configuration ===============
epochs: 100                          # Total training epochs
batch_size: 4                        # Batch size (note: actual batch is list due to variable point counts)
num_workers: 4                       # DataLoader workers

# Optimizer
optimizer: 'adamw'                   # adamw, adam, or sgd
lr: 0.00005                           # Learning rate (5e-4)
weight_decay: 0.0001                 # Weight decay (1e-4)
grad_clip: 1.0                       # Gradient clipping (0 to disable)

# Learning rate scheduler
scheduler: 'cosine'                  # cosine, step, plateau, or null
min_lr: 0.000001                     # Minimum learning rate for cosine scheduler (1e-6)
lr_decay_step: 20                    # Step size for step scheduler
lr_decay_gamma: 0.5                  # Decay factor for step scheduler

# =============== Logging & Checkpointing ===============
output_dir: './icp_train_output_lowlr'     # Output directory
log_freq: 10                         # Log to TensorBoard every N steps
val_freq: 1                          # Validate every N epochs
save_freq: 10                        # Save checkpoint every N epochs

# Resume from checkpoint
resume_from: null                    # Path to checkpoint to resume from (null = train from scratch)

# =============== Device Configuration ===============
device: 'cuda:0'                     # PyTorch device
