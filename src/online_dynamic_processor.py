# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""
åœ¨çº¿åŠ¨æ€ç‰©ä½“å¤„ç†å™¨

å®æ—¶è¿›è¡ŒåŠ¨æ€ç‰©ä½“æ£€æµ‹ã€åˆ†å‰²ã€è·Ÿè¸ªå’Œèšåˆ
ä½¿ç”¨demo_video_with_pointcloud_save.pyå’Œoptical_flow_registration.pyä¸­çš„æˆç†Ÿæ–¹æ³•
"""

import torch
import torch.nn.functional as F
import numpy as np
import cv2
from typing import Dict, List, Optional, Any, Tuple
import time
from collections import defaultdict

# å¯¼å…¥ç°æœ‰çš„èšç±»å’Œå…‰æµé…å‡†ç³»ç»Ÿ
import sys
import os
from sklearn.cluster import DBSCAN
from scipy.optimize import linear_sum_assignment

# å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
def _import_clustering_functions():
    """å»¶è¿Ÿå¯¼å…¥èšç±»å‡½æ•°"""
    try:
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        # å¯¼å…¥å•ç‹¬çš„å‡½æ•°è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å—
        import importlib.util
        
        # å¯¼å…¥èšç±»æ–¹æ³•
        demo_spec = importlib.util.spec_from_file_location(
            "demo_clustering", 
            os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "demo_video_with_pointcloud_save.py")
        )
        demo_module = importlib.util.module_from_spec(demo_spec)
        demo_spec.loader.exec_module(demo_module)
        
        return demo_module.dynamic_object_clustering, demo_module.match_objects_across_frames
    except Exception as e:
        print(f"Failed to import clustering functions: {e}")
        return None, None

def _import_optical_flow():
    """å»¶è¿Ÿå¯¼å…¥å…‰æµé…å‡†"""
    try:
        import importlib.util
        import sys
        
        # æ·»åŠ æ ¹ç›®å½•åˆ°sys.path
        root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if root_dir not in sys.path:
            sys.path.insert(0, root_dir)
            
        optical_flow_spec = importlib.util.spec_from_file_location(
            "optical_flow_reg", 
            os.path.join(root_dir, "optical_flow_registration.py")
        )
        optical_flow_module = importlib.util.module_from_spec(optical_flow_spec)
        optical_flow_spec.loader.exec_module(optical_flow_module)
        
        return optical_flow_module.OpticalFlowRegistration
    except Exception as e:
        print(f"Failed to import optical flow: {e}")
        return None


class OnlineDynamicProcessor:
    """
    åœ¨çº¿åŠ¨æ€ç‰©ä½“å¤„ç†å™¨
    
    å®æ—¶æ‰§è¡Œï¼š
    1. åŠ¨æ€ç‰©ä½“æ£€æµ‹å’Œèšç±»ï¼ˆä½¿ç”¨demo_video_with_pointcloud_save.pyä¸­çš„æ–¹æ³•ï¼‰
    2. è·¨å¸§ç‰©ä½“è·Ÿè¸ªï¼ˆä½¿ç”¨æˆç†Ÿçš„åŒ¹é…ç®—æ³•ï¼‰
    3. å…‰æµé…å‡†å’Œèšåˆï¼ˆä½¿ç”¨optical_flow_registration.pyä¸­çš„æ–¹æ³•ï¼‰
    4. ç¬¬äºŒé˜¶æ®µè®­ç»ƒæ•°æ®å‡†å¤‡
    """
    
    def __init__(
        self,
        device: torch.device,
        memory_efficient: bool = True,
        min_object_size: int = 100,
        max_objects_per_frame: int = 10,
        velocity_threshold_percentile: float = 0.75,
        iou_threshold: float = 0.3,
        use_optical_flow_aggregation: bool = True,
        enable_temporal_cache: bool = True,
        cache_size: int = 16
    ):
        """
        åˆå§‹åŒ–åœ¨çº¿åŠ¨æ€ç‰©ä½“å¤„ç†å™¨
        
        Args:
            device: è®¡ç®—è®¾å¤‡
            memory_efficient: æ˜¯å¦å¯ç”¨å†…å­˜ä¼˜åŒ–
            min_object_size: æœ€å°ç‰©ä½“å°ºå¯¸ï¼ˆç‚¹æ•°ï¼‰
            max_objects_per_frame: æ¯å¸§æœ€å¤§ç‰©ä½“æ•°é‡
            velocity_threshold_percentile: é€Ÿåº¦é˜ˆå€¼ç™¾åˆ†ä½æ•°
            iou_threshold: IoUåŒ¹é…é˜ˆå€¼
            use_optical_flow_aggregation: æ˜¯å¦ä½¿ç”¨å…‰æµèšåˆ
            enable_temporal_cache: æ˜¯å¦å¯ç”¨æ—¶åºç¼“å­˜
            cache_size: ç¼“å­˜å¤§å°
        """
        self.device = device
        self.memory_efficient = memory_efficient
        self.min_object_size = min_object_size
        self.max_objects_per_frame = max_objects_per_frame
        self.velocity_threshold_percentile = velocity_threshold_percentile
        self.iou_threshold = iou_threshold
        self.use_optical_flow_aggregation = use_optical_flow_aggregation
        
        # åˆå§‹åŒ–å…‰æµé…å‡†ç³»ç»Ÿ
        self.optical_flow_registration = None
        self._optical_flow_class = None
        if use_optical_flow_aggregation:
            self._optical_flow_class = _import_optical_flow()
            # ç«‹å³åˆå§‹åŒ–å…‰æµé…å‡†ç³»ç»Ÿ
            if self._optical_flow_class is not None:
                try:
                    self.optical_flow_registration = self._optical_flow_class(
                        device=str(device),
                        use_pnp=True,
                        min_inliers_ratio=0.1,  # é™ä½æœ€å°å†…ç‚¹æ¯”ä¾‹
                        ransac_threshold=5.0,   # å¢åŠ RANSACé˜ˆå€¼  
                        max_flow_magnitude=200.0  # å¢åŠ æœ€å¤§å…‰æµå¹…åº¦
                    )
                except Exception as e:
                    self.optical_flow_registration = None
        
        # ç¼“å­˜èšç±»å‡½æ•°
        self._dynamic_clustering_func = None
        self._match_objects_func = None
        
        # æ—¶åºç¼“å­˜
        self.enable_temporal_cache = enable_temporal_cache
        self.cache_size = cache_size
        self.temporal_cache = defaultdict(list) if enable_temporal_cache else None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.processing_stats = {
            'total_sequences': 0,
            'total_objects_detected': 0,
            'total_processing_time': 0.0,
            'sam_time': 0.0,
            'optical_flow_time': 0.0,
            'aggregation_time': 0.0
        }
        
    
    def process_dynamic_objects(
        self,
        preds: Dict[str, Any],
        vggt_batch: Dict[str, Any],
        auxiliary_models: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¤„ç†åŠ¨æ€ç‰©ä½“çš„ä¸»è¦æ¥å£
        
        Args:
            preds: VGGTæ¨¡å‹é¢„æµ‹ç»“æœ
            vggt_batch: VGGTæ‰¹æ¬¡æ•°æ®
            auxiliary_models: è¾…åŠ©æ¨¡å‹ï¼ˆå¦‚SAM2ï¼‰
            
        Returns:
            åŒ…å«åŠ¨æ€ç‰©ä½“å’Œé™æ€èƒŒæ™¯çš„å­—å…¸
        """
        start_time = time.time()
        stage_times = {}
        
        try:
            # è·å–åŸºæœ¬ä¿¡æ¯
            images = vggt_batch.get('images')  # [B, S, 3, H, W]
            if images is None:
                return {'dynamic_objects': [], 'static_gaussians': None}
            
            B, S, C, H, W = images.shape
            velocity = preds.get('velocity')  # [B, S, H, W, 3]
            gaussian_params = preds.get('gaussian_params')  # [B, S*H*W, 14] or similar
            
            # ========== Stage 1: æ•°æ®åå¤„ç† ==========
            preprocessing_start = time.time()
            
            # 1. Velocityåå¤„ç†
            if velocity is not None:
                # åº”ç”¨é€Ÿåº¦åå¤„ç†ï¼švelocity = sign(velocity) * (exp(|velocity|) - 1)
                velocity_processed = torch.sign(velocity) * (torch.exp(torch.abs(velocity)) - 1)
                
                # å¦‚æœæœ‰ç›¸æœºå‚æ•°ï¼Œå°†é€Ÿåº¦ä»å±€éƒ¨åæ ‡è½¬æ¢åˆ°å…¨å±€åæ ‡
                if 'pose_enc' in preds:
                    from vggt.training.loss import velocity_local_to_global
                    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
                    
                    # è·å–é¢„æµ‹çš„ç›¸æœºå‚æ•°
                    extrinsics, intrinsics = pose_encoding_to_extri_intri(
                        preds["pose_enc"], vggt_batch["images"].shape[-2:]
                    )
                    # æ·»åŠ é½æ¬¡åæ ‡è¡Œ
                    extrinsics = torch.cat([
                        extrinsics, 
                        torch.tensor([0, 0, 0, 1], device=extrinsics.device)[None, None, None, :].repeat(1, extrinsics.shape[1], 1, 1)
                    ], dim=-2)
                    
                    # è½¬æ¢é€Ÿåº¦åæ ‡ç³»
                    extrinsic_inv = torch.linalg.inv(extrinsics)  # [B, S, 4, 4]
                    
                    # Reshape velocity for coordinate transformation
                    if len(velocity_processed.shape) == 5:  # [B, S, H, W, 3]
                        B_v, S_v, H_v, W_v, _ = velocity_processed.shape
                        velocity_flat = velocity_processed.reshape(B_v, S_v * H_v * W_v, 3)  # [B, S*H*W, 3]
                        
                        # Transform each batch separately
                        velocity_transformed = []
                        for b in range(B_v):
                            vel_b = velocity_flat[b].reshape(-1, 3)  # [S*H*W, 3]
                            vel_transformed = velocity_local_to_global(vel_b, extrinsic_inv[b:b+1])
                            velocity_transformed.append(vel_transformed.reshape(S_v, H_v, W_v, 3))
                        
                        velocity_processed = torch.stack(velocity_transformed, dim=0)  # [B, S, H, W, 3]
                
            # å¦‚æœæœ‰æ•°æ®éœ€è¦åå¤„ç†ï¼Œå¤åˆ¶predsé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            preds_updated = preds.copy()
            
            if velocity is not None:
                # æ›´æ–°å¤„ç†åçš„velocity
                preds_updated['velocity'] = velocity_processed
                velocity = velocity_processed
                
            # 2. Gaussianå‚æ•°åå¤„ç†
            if gaussian_params is not None:
                # é‡æ–°æ•´å½¢gaussianå‚æ•°ä¸º [S, H*W, 14]
                if gaussian_params.dim() == 3 and gaussian_params.shape[1] == S * H * W:
                    # æƒ…å†µ1: [B, S*H*W, 14] -> [S, H*W, 14]
                    gaussian_params_reshaped = gaussian_params[0].reshape(S, H * W, 14)
                elif gaussian_params.dim() == 3 and gaussian_params.shape[0] == S:
                    # æƒ…å†µ2: [S, H*W, 14] -> å·²ç»æ˜¯æ­£ç¡®å½¢çŠ¶
                    gaussian_params_reshaped = gaussian_params
                else:
                    # å…¶ä»–æƒ…å†µï¼Œå°è¯•é‡æ–°æ•´å½¢
                    gaussian_params_reshaped = gaussian_params.reshape(S, H * W, 14)
                
                # åº”ç”¨gaussianå‚æ•°æ¿€æ´»å‡½æ•°
                gaussian_params_processed = self._apply_gaussian_activation(gaussian_params_reshaped)
                
                # ç”¨depthè®¡ç®—çš„3Dåæ ‡æ›¿æ¢gaussian_paramsçš„å‰ä¸‰ç»´ï¼ˆå‚è€ƒflow_losså‡½æ•°ï¼‰
                depth_data = preds.get('depth')
                if depth_data is not None and 'pose_enc' in preds:
                    try:
                        from vggt.training.loss import depth_to_world_points
                        from vggt.utils.pose_enc import pose_encoding_to_extri_intri
                        
                        # è·å–ç›¸æœºå‚æ•°
                        extrinsics, intrinsics = pose_encoding_to_extri_intri(
                            preds["pose_enc"], images.shape[-2:]
                        )
                        extrinsics = torch.cat([
                            extrinsics, 
                            torch.tensor([0, 0, 0, 1], device=extrinsics.device)[None, None, None, :].repeat(1, extrinsics.shape[1], 1, 1)
                        ], dim=-2)
                        
                        # è®¡ç®—world pointsï¼ˆä¸flow_lossä¸­ç›¸åŒçš„é€»è¾‘ï¼‰
                        depth_for_points = depth_data.reshape(B*S, H, W, 1)
                        world_points = depth_to_world_points(depth_for_points, intrinsics)
                        world_points = world_points.reshape(world_points.shape[0], world_points.shape[1]*world_points.shape[2], 3)
                        
                        # è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»
                        extrinsic_inv = torch.linalg.inv(extrinsics)
                        xyz_camera = torch.matmul(extrinsic_inv[0, :, :3, :3], world_points.transpose(-1, -2)).transpose(-1, -2) + \
                                    extrinsic_inv[0, :, :3, 3:4].transpose(-1, -2)
                        xyz_camera = xyz_camera.reshape(S, H * W, 3)  # [S, H*W, 3]
                        
                        # æ›¿æ¢gaussian_paramsçš„å‰ä¸‰ç»´
                        gaussian_params_processed[:, :, :3] = xyz_camera
                        
                    except Exception as e:
                        pass  # é™é»˜å¤„ç†é”™è¯¯
                
                # æ›´æ–°å¤„ç†åçš„gaussian_paramsï¼ˆä¿æŒåŸå§‹æ ¼å¼ï¼‰
                preds_updated['gaussian_params'] = gaussian_params_processed.unsqueeze(0).reshape(B, S * H * W, 14)
                gaussian_params = preds_updated['gaussian_params']
            
            # ä½¿ç”¨æ›´æ–°åçš„preds
            preds = preds_updated
            stage_times['preprocessing'] = time.time() - preprocessing_start
            
            # ========== Stage 2: åŠ¨æ€ç‰©ä½“èšç±» ==========
            clustering_start = time.time()
            clustering_results = self._perform_clustering_with_existing_method(
                preds, vggt_batch, velocity
            )
            stage_times['Stage 2: åŠ¨æ€ç‰©ä½“èšç±»'] = time.time() - clustering_start
            
            # ========== Stage 3: è·¨å¸§ç‰©ä½“è·Ÿè¸ª ==========
            tracking_start = time.time()
            if self._match_objects_func is None:
                _, self._match_objects_func = _import_clustering_functions()
            
            if self._match_objects_func is not None:
                matched_clustering_results = self._match_objects_func(
                    clustering_results, 
                    position_threshold=0.5, 
                    velocity_threshold=0.2
                )
            else:
                matched_clustering_results = clustering_results
            stage_times['Stage 3: è·¨å¸§ç‰©ä½“è·Ÿè¸ª'] = time.time() - tracking_start
            
            # ========== Stage 4: å…‰æµèšåˆ ==========
            aggregation_start = time.time()
            dynamic_objects = []
            if self.optical_flow_registration is not None and len(matched_clustering_results) > 0:
                try:
                    dynamic_objects = self._aggregate_with_existing_optical_flow_method(
                        matched_clustering_results, preds, vggt_batch
                    )
                except Exception as e:
                    print(f"å…‰æµèšåˆå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•: {e}")
                    dynamic_objects = self._create_objects_from_clustering_results(
                        matched_clustering_results, gaussian_params, H, W, preds
                    )
            else:
                dynamic_objects = self._create_objects_from_clustering_results(
                    matched_clustering_results, gaussian_params, H, W, preds
                )
            stage_times['Stage 4: å…‰æµèšåˆ'] = time.time() - aggregation_start
            
            # ========== Stage 5: èƒŒæ™¯åˆ†ç¦» ==========
            background_start = time.time()
            static_gaussians = self._create_static_background(
                preds, velocity, matched_clustering_results, H, W, S
            )
            stage_times['Stage 5: èƒŒæ™¯åˆ†ç¦»'] = time.time() - background_start
            
            # æ˜¾ç¤ºå„é˜¶æ®µè€—æ—¶
            total_time = time.time() - start_time
            for stage_name, stage_time in stage_times.items():
                print(f"  {stage_name}: {stage_time:.4f}s")
            print(f"  æ€»è€—æ—¶: {total_time:.4f}s")
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.processing_stats['total_sequences'] += 1
            self.processing_stats['total_objects_detected'] += len(dynamic_objects)
            self.processing_stats['total_processing_time'] += total_time
            
            # å†…å­˜æ¸…ç†
            if self.memory_efficient:
                torch.cuda.empty_cache()
            
            return {
                'dynamic_objects': dynamic_objects,
                'static_gaussians': static_gaussians,
                'processing_time': total_time,
                'num_objects': len(dynamic_objects),
                'stage_times': stage_times
            }
            
        except Exception as e:
            print(f"Error in dynamic object processing: {e}")
            return {'dynamic_objects': [], 'static_gaussians': None, 'stage_times': {}}
    
    def _perform_clustering_with_existing_method(
        self,
        preds: Dict[str, Any],
        vggt_batch: Dict[str, Any],
        velocity: Optional[torch.Tensor]
    ) -> List[Dict]:
        """ä½¿ç”¨demo_video_with_pointcloud_save.pyä¸­çš„èšç±»æ–¹æ³•"""
        try:
            # ä»VGGTé¢„æµ‹ç»“æœä¸­æå–ç‚¹äº‘åæ ‡
            if 'depth' in preds and 'pose_enc' in preds:
                # ä½¿ç”¨é¢„æµ‹çš„ç›¸æœºå‚æ•°ï¼ˆä¸demoä¸€è‡´ï¼‰
                from vggt.training.loss import depth_to_world_points, velocity_local_to_global
                from vggt.utils.pose_enc import pose_encoding_to_extri_intri
                
                depths = preds['depth']  # å¯èƒ½æ˜¯ [B, S, H, W] æˆ– [S, H, W, 1]
                # è·å–é¢„æµ‹çš„ç›¸æœºå‚æ•°
                pose_result = pose_encoding_to_extri_intri(
                    preds["pose_enc"], vggt_batch["images"].shape[-2:]
                )
                if len(pose_result) != 2:
                    raise ValueError(f"pose_encoding_to_extri_intri returned {len(pose_result)} values, expected 2")
                extrinsics, intrinsics = pose_result
                # æ·»åŠ é½æ¬¡åæ ‡è¡Œ
                extrinsics = torch.cat([
                    extrinsics, 
                    torch.tensor([0, 0, 0, 1], device=extrinsics.device)[None, None, None, :].repeat(1, extrinsics.shape[1], 1, 1)
                ], dim=-2)
                
                # å¤„ç†ä¸åŒçš„depthå½¢çŠ¶
                if len(depths.shape) == 5 and depths.shape[-1] == 1:
                    # å½¢çŠ¶ä¸º [B, S, H, W, 1]ï¼Œè½¬æ¢ä¸º [B, S, H, W]
                    B, S, H, W, _ = depths.shape
                    depths = depths.squeeze(-1)  # [B, S, H, W]
                elif len(depths.shape) == 4 and depths.shape[-1] == 1:
                    # å½¢çŠ¶ä¸º [S, H, W, 1]ï¼Œè½¬æ¢ä¸º [B, S, H, W]
                    S, H, W, _ = depths.shape
                    B = 1
                    depths = depths.squeeze(-1).unsqueeze(0)  # [B, S, H, W]
                elif len(depths.shape) == 4:
                    # å·²ç»æ˜¯ [B, S, H, W] æ ¼å¼
                    B, S, H, W = depths.shape
                else:
                    raise ValueError(f"Unexpected depth shape: {depths.shape}")
                
                
                # è®¡ç®—ä¸–ç•Œåæ ‡ç‚¹äº‘
                # å…ˆæ·»åŠ æœ€åä¸€ä¸ªç»´åº¦åˆ°depthä»¥åŒ¹é…å‡½æ•°æœŸæœ›çš„[N, H, W, 1]æ ¼å¼
                depth_with_dim = depths.reshape(B*S, H, W, 1)  # [B*S, H, W, 1]
                xyz_world = depth_to_world_points(
                    depth_with_dim, 
                    intrinsics.reshape(B*S, 3, 3)
                )  # [B*S, H*W, 3]
                
                # é‡æ–°æ•´å½¢ä¸º [B, S, H*W, 3]
                xyz_world = xyz_world.reshape(B, S, H*W, 3)
                
                # è½¬æ¢åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆä¸demoä¸€è‡´ï¼‰
                extrinsic_inv = torch.linalg.inv(extrinsics)  # [B, S, 4, 4]
                xyz_world_flat = xyz_world[0]  # [S, H*W, 3]
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªbatchçš„extrinsic_invè¿›è¡Œåæ ‡å˜æ¢
                extrinsic_inv_first = extrinsic_inv[0]  # [S, 4, 4]
                xyz = torch.matmul(extrinsic_inv_first[:, :3, :3], xyz_world_flat.transpose(-1, -2)).transpose(-1, -2) + \
                      extrinsic_inv_first[:, :3, 3:4].transpose(-1, -2)  # [S, H*W, 3]
                
            else:
                B, S = velocity.shape[0], velocity.shape[1] if velocity is not None else 4
                H, W = 224, 224  # é»˜è®¤å°ºå¯¸
                xyz = torch.zeros(S, H*W, 3, device=self.device)
            
            # å¤„ç†é€Ÿåº¦ä¿¡æ¯ï¼ˆå·²ç»åœ¨å‡½æ•°å¼€å¤´å¤„ç†è¿‡äº†ï¼Œè¿™é‡Œåªéœ€è¦reshapeï¼‰
            if velocity is not None:
                if len(velocity.shape) == 5:
                    B, S, H, W, _ = velocity.shape
                    velocity_reshaped = velocity[0].reshape(S, H*W, 3)  # å–ç¬¬ä¸€ä¸ªbatch: [S, H*W, 3]
                elif len(velocity.shape) == 4:
                    # Handle case where velocity is [B, S, H*W, 3] format  
                    B, S, HW, _ = velocity.shape
                    velocity_reshaped = velocity[0]  # [S, H*W, 3]
                else:
                    raise ValueError(f"Unexpected velocity shape: {velocity.shape}")
                # æ³¨æ„ï¼šé€Ÿåº¦åå¤„ç†å’Œåæ ‡å˜æ¢å·²ç»åœ¨process_dynamic_objectså¼€å¤´å®Œæˆ
            else:
                velocity_reshaped = torch.zeros(S, H*W, 3, device=self.device)
            
            # ä½¿ç”¨ç°æœ‰çš„åŠ¨æ€ç‰©ä½“èšç±»å‡½æ•°
            if self._dynamic_clustering_func is None:
                self._dynamic_clustering_func, _ = _import_clustering_functions()
            
            if self._dynamic_clustering_func is not None:
                # åˆ†ç¦»å¼ é‡æ¢¯åº¦ä»¥ä¾¿åœ¨èšç±»ä¸­ä½¿ç”¨numpy
                xyz_detached = xyz.detach()
                velocity_detached = velocity_reshaped.detach()
                
                clustering_results = self._dynamic_clustering_func(
                    xyz_detached,  # [S, H*W, 3]
                    velocity_detached,  # [S, H*W, 3]
                    velocity_threshold=0.01,  # é€Ÿåº¦é˜ˆå€¼
                    eps=0.02,  # DBSCANçš„é‚»åŸŸåŠå¾„
                    min_samples=10,  # DBSCANçš„æœ€å°æ ·æœ¬æ•°
                    area_threshold=self.min_object_size  # é¢ç§¯é˜ˆå€¼
                )
            else:
                # å›é€€åˆ°ç®€å•å®ç°
                clustering_results = self._simple_clustering(xyz, velocity_reshaped)
            
            return clustering_results
            
        except Exception as e:
            # è¿”å›ç©ºçš„èšç±»ç»“æœ
            return []
    
    def _simple_clustering(self, xyz: torch.Tensor, velocity: torch.Tensor) -> List[Dict]:
        """ç®€å•çš„èšç±»å®ç°ï¼ˆä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            clustering_results = []
            S = xyz.shape[0]
            
            for frame_idx in range(S):
                frame_points = xyz[frame_idx]  # [H*W, 3]
                frame_velocity = velocity[frame_idx]  # [H*W, 3]
                
                # è®¡ç®—é€Ÿåº¦å¤§å°
                velocity_magnitude = torch.norm(frame_velocity, dim=-1)  # [H*W]
                
                # è¿‡æ»¤åŠ¨æ€ç‚¹
                velocity_threshold = 0.01
                dynamic_mask = velocity_magnitude > velocity_threshold
                dynamic_points = frame_points[dynamic_mask]
                
                if len(dynamic_points) < 10:
                    clustering_results.append({
                        'frame_idx': frame_idx,  # æ·»åŠ frame_idxå­—æ®µ
                        'points': frame_points,
                        'labels': torch.full((len(frame_points),), -1, dtype=torch.long),
                        'dynamic_mask': dynamic_mask,
                        'num_clusters': 0,
                        'cluster_centers': [],
                        'cluster_velocities': [],
                        'cluster_sizes': [],
                        'global_ids': [],
                        'cluster_indices': []  # æ·»åŠ cluster_indiceså­—æ®µ
                    })
                    continue
                
                # ç®€å•çš„åŸºäºç©ºé—´çš„èšç±»
                dynamic_points_np = dynamic_points.cpu().numpy()
                
                try:
                    # ä½¿ç”¨DBSCANèšç±»
                    dbscan = DBSCAN(eps=0.02, min_samples=10)
                    cluster_labels = dbscan.fit_predict(dynamic_points_np)
                    
                    # æ˜ å°„å›åŸå§‹ç‚¹äº‘
                    full_labels = torch.full((len(frame_points),), -1, dtype=torch.long)
                    full_labels[dynamic_mask] = torch.from_numpy(cluster_labels)
                    
                    # ç»Ÿè®¡èšç±»ä¿¡æ¯
                    unique_labels = set(cluster_labels)
                    if -1 in unique_labels:
                        unique_labels.remove(-1)
                    
                    num_clusters = len(unique_labels)
                    cluster_centers = []
                    cluster_velocities = []
                    cluster_sizes = []
                    
                    for label in sorted(unique_labels):
                        cluster_mask = cluster_labels == label
                        cluster_points = dynamic_points[cluster_mask]
                        if len(cluster_points) >= self.min_object_size:
                            center = cluster_points.mean(dim=0)
                            cluster_centers.append(center)
                            cluster_velocities.append(frame_velocity[dynamic_mask][cluster_mask].mean(dim=0))
                            cluster_sizes.append(len(cluster_points))
                    
                    # æ„å»ºcluster_indices - æ¯ä¸ªèšç±»ä¸­å¿ƒå¯¹åº”çš„åƒç´ ç´¢å¼•åˆ—è¡¨
                    cluster_indices = []
                    H_W = len(frame_points)
                    for label in range(len(cluster_centers)):
                        # æ‰¾åˆ°å±äºè¯¥èšç±»çš„æ‰€æœ‰ç‚¹çš„ç´¢å¼•
                        cluster_mask = (full_labels == label)
                        indices = torch.where(cluster_mask)[0].tolist()
                        cluster_indices.append(indices)
                    
                    clustering_results.append({
                        'frame_idx': frame_idx,  # æ·»åŠ frame_idxå­—æ®µ
                        'points': frame_points,
                        'labels': full_labels,
                        'dynamic_mask': dynamic_mask,
                        'num_clusters': len(cluster_centers),
                        'cluster_centers': cluster_centers,
                        'cluster_velocities': cluster_velocities,
                        'cluster_sizes': cluster_sizes,
                        'global_ids': list(range(len(cluster_centers))),  # ç®€å•åˆ†é…ID
                        'cluster_indices': cluster_indices  # æ·»åŠ æ¯ä¸ªèšç±»å¯¹åº”çš„åƒç´ ç´¢å¼•åˆ—è¡¨
                    })
                    
                except Exception as e:
                    clustering_results.append({
                        'frame_idx': frame_idx,  # æ·»åŠ frame_idxå­—æ®µ
                        'points': frame_points,
                        'labels': torch.full((len(frame_points),), -1, dtype=torch.long),
                        'dynamic_mask': dynamic_mask,
                        'num_clusters': 0,
                        'cluster_centers': [],
                        'cluster_velocities': [],
                        'cluster_sizes': [],
                        'global_ids': [],
                        'cluster_indices': []  # æ·»åŠ cluster_indiceså­—æ®µ
                    })
            
            return clustering_results
            
        except Exception as e:
            return []
    
    def _aggregate_with_existing_optical_flow_method(
        self,
        clustering_results: List[Dict],
        preds: Dict[str, Any],
        vggt_batch: Dict[str, Any]
    ) -> List[Dict]:
        """ä½¿ç”¨optical_flow_registration.pyä¸­çš„å…‰æµèšåˆæ–¹æ³•"""
        import time
        method_start = time.time()
        
        try:
            if self.optical_flow_registration is None:
                # è·å–å›¾åƒå°ºå¯¸
                H, W = 64, 64  # é»˜è®¤å€¼ï¼Œå®é™…åº”è¯¥ä»clusteringç»“æœæˆ–å…¶ä»–åœ°æ–¹è·å–
                if clustering_results and len(clustering_results) > 0:
                    points = clustering_results[0].get('points')
                    if points is not None and len(points.shape) >= 2:
                        # å‡è®¾pointsæ˜¯[H*W, 3]æ ¼å¼ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä»åˆ«å¤„è·å–H, W
                        # è¿™é‡Œä½¿ç”¨é»˜è®¤å€¼ï¼Œå®é™…æƒ…å†µå¯èƒ½éœ€è¦è°ƒæ•´
                        pass
                return self._create_objects_from_clustering_results(
                    clustering_results, None, H, W
                )
            
            # 1. é¢„è®¡ç®—æ‰€æœ‰å¸§ä¹‹é—´çš„å…‰æµ
            flow_start = time.time()
            flows = self.optical_flow_registration.precompute_optical_flows(vggt_batch)
            flow_time = time.time() - flow_start
            print(f"    é¢„è®¡ç®—å…‰æµè€—æ—¶: {flow_time:.4f}s")
            
            # 2. è·å–æ‰€æœ‰å…¨å±€ç‰©ä½“ID
            ids_start = time.time()
            all_global_ids = set()
            for result in clustering_results:
                all_global_ids.update(result.get('global_ids', []))
            ids_time = time.time() - ids_start
            print(f"    è·å–å…¨å±€ç‰©ä½“IDè€—æ—¶: {ids_time:.4f}s ({len(all_global_ids)} ä¸ªç‰©ä½“)")
            
            dynamic_objects = []
            
            # 3. å¯¹æ¯ä¸ªå…¨å±€ç‰©ä½“è¿›è¡Œå…‰æµèšåˆ
            aggregation_start = time.time()
            for i, global_id in enumerate(all_global_ids):
                object_start = time.time()
                try:
                    aggregated_object = self.optical_flow_registration.aggregate_object_to_middle_frame(
                        clustering_results, preds, vggt_batch, global_id, flows
                    )
                    object_time = time.time() - object_start
                    print(f"    ç‰©ä½“ {global_id} ({i+1}/{len(all_global_ids)}) èšåˆè€—æ—¶: {object_time:.4f}s")
                    
                    if aggregated_object is not None:
                        print(f"ç‰©ä½“ {global_id}: èšåˆæˆåŠŸï¼ŒåŒ…å« {len(aggregated_object.get('aggregated_points', []))} ä¸ªç‚¹")
                        
                        # ä½¿ç”¨aggregate_object_to_middle_frameå·²ç»æå–çš„canonical_gaussians
                        aggregated_gaussians = aggregated_object.get('canonical_gaussians')
                        
                        if aggregated_gaussians is not None:
                            print(f"ç‰©ä½“ {global_id}: æˆåŠŸè·å–åˆ° {aggregated_gaussians.shape[0]} ä¸ªGaussianå‚æ•°")
                        else:
                            print(f"ç‰©ä½“ {global_id}: âš ï¸ æœªèƒ½è·å–Gaussianå‚æ•°")
                        
                        # è·å–å˜æ¢ä¿¡æ¯
                        reference_frame = aggregated_object.get('middle_frame', 0)  # ä¿®æ­£ï¼šä½¿ç”¨middle_frame
                        transformations = aggregated_object.get('transformations', {})  # å„å¸§åˆ°reference_frameçš„å˜æ¢
                        object_frames = aggregated_object.get('object_frames', [])
                        
                        # æ„å»ºframe_transformså­—å…¸ï¼ˆStage2LossæœŸæœ›çš„æ ¼å¼ï¼‰
                        frame_transforms = {}
                        for frame_idx in object_frames:
                            if frame_idx in transformations:
                                # ç›´æ¥ä½¿ç”¨å…‰æµèšåˆå™¨è®¡ç®—çš„å˜æ¢çŸ©é˜µ
                                transform_data = transformations[frame_idx]
                                if isinstance(transform_data, dict) and 'transformation' in transform_data:
                                    transform = transform_data['transformation']
                                else:
                                    transform = transform_data
                                
                                # è½¬æ¢ä¸ºtorch tensor
                                if isinstance(transform, np.ndarray):
                                    transform = torch.from_numpy(transform).to(self.device).float()
                                
                                # å…³é”®ä¿®å¤ï¼šéªŒè¯å˜æ¢çŸ©é˜µï¼Œé˜²æ­¢å¤§ç™½çƒé—®é¢˜
                                if self._validate_and_fix_transform(transform, frame_idx, global_id):
                                    frame_transforms[frame_idx] = transform
                                else:
                                    print(f"è·³è¿‡å¯¹è±¡{global_id}åœ¨å¸§{frame_idx}çš„å¼‚å¸¸å˜æ¢çŸ©é˜µ")
                            elif frame_idx == reference_frame:
                                # reference_frameåˆ°è‡ªå·±çš„å˜æ¢æ˜¯æ’ç­‰å˜æ¢
                                frame_transforms[frame_idx] = torch.eye(4, device=self.device)
                        
                        # åˆ›å»ºframe_existenceæ ‡è®°
                        max_frame = max(object_frames) if object_frames else reference_frame
                        frame_existence = []
                        for frame_idx in range(max_frame + 1):
                            frame_existence.append(frame_idx in object_frames)
                        
                        # è½¬æ¢ä¸ºæˆ‘ä»¬éœ€è¦çš„æ ¼å¼ - ç›´æ¥æ„å»ºStage2LossæœŸæœ›çš„ç»“æ„
                        dynamic_objects.append({
                            'object_id': global_id,
                            'canonical_gaussians': aggregated_gaussians,  # canonicalç©ºé—´çš„é«˜æ–¯å‚æ•°
                            'reference_frame': reference_frame,  # æ­£è§„ç©ºé—´ä½äºç¬¬å‡ å¸§
                            'frame_transforms': frame_transforms,  # å…¶ä»–å¸§å’Œæ­£è§„ç©ºé—´å¸§çš„transform
                            'frame_existence': torch.tensor(frame_existence, dtype=torch.bool, device=self.device),
                            # ä¿ç•™åŸå§‹æ•°æ®ä¾›è°ƒè¯•ä½¿ç”¨
                            'aggregated_points': aggregated_object.get('aggregated_points'),
                            'aggregated_colors': aggregated_object.get('aggregated_colors'),
                            'transformations': transformations,  # åŸå§‹å˜æ¢æ•°æ®
                        })
                    else:
                        print(f"ç‰©ä½“ {global_id}: èšåˆå¤±è´¥ï¼Œaggregated_objectä¸ºNone")
                except Exception as e:
                    print(f"ç‰©ä½“ {global_id}: èšåˆè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
                    import traceback
                    traceback.print_exc()
            
            aggregation_total_time = time.time() - aggregation_start
            method_total_time = time.time() - method_start
            
            print(f"    ç‰©ä½“èšåˆæ€»è€—æ—¶: {aggregation_total_time:.4f}s")
            print(f"    å…‰æµèšåˆæ–¹æ³•æ€»è€—æ—¶: {method_total_time:.4f}s")
            print(f"    æ€§èƒ½åˆ†æï¼šé¢„è®¡ç®—å…‰æµ({flow_time:.3f}s) + è·å–ID({ids_time:.3f}s) + ç‰©ä½“èšåˆ({aggregation_total_time:.3f}s)")
            print(f"    å…‰æµèšåˆå®Œæˆ: å¤„ç†äº† {len(all_global_ids)} ä¸ªç‰©ä½“ï¼ŒæˆåŠŸèšåˆ {len(dynamic_objects)} ä¸ªç‰©ä½“")
            return dynamic_objects
            
        except Exception as e:
            # ä½¿ç”¨é»˜è®¤å°ºå¯¸
            H, W = 64, 64
            return self._create_objects_from_clustering_results(clustering_results, None, H, W)
    
    def _create_objects_from_clustering_results(
        self,
        clustering_results: List[Dict],
        gaussian_params: Optional[torch.Tensor] = None,
        H: int = None,
        W: int = None,
        preds: Optional[Dict] = None
    ) -> List[Dict]:
        """ä»èšç±»ç»“æœåˆ›å»ºåŠ¨æ€ç‰©ä½“ï¼ˆæ— å…‰æµèšåˆï¼‰"""
        dynamic_objects = []
        
        try:
            # è·å–æ‰€æœ‰å…¨å±€ç‰©ä½“ID
            all_global_ids = set()
            for result in clustering_results:
                all_global_ids.update(result.get('global_ids', []))
            
            for global_id in all_global_ids:
                # æ”¶é›†è¯¥ç‰©ä½“åœ¨æ‰€æœ‰å¸§ä¸­çš„ç‚¹äº‘å’Œç´¢å¼•
                object_points = []
                object_frames = []
                object_indices = []
                
                for frame_idx, result in enumerate(clustering_results):
                    global_ids = result.get('global_ids', [])
                    if global_id in global_ids:
                        cluster_idx = global_ids.index(global_id)
                        points = result['points']
                        labels = result['labels']
                        cluster_indices = result.get('cluster_indices', [])
                        
                        # æå–ç‰©ä½“ç‚¹äº‘
                        object_mask = labels == cluster_idx
                        frame_object_points = points[object_mask]
                        
                        # æå–å¯¹åº”çš„ç‚¹ç´¢å¼•ï¼ˆä»VGGTé¢„æµ‹ä¸­æå–gaussianå‚æ•°éœ€è¦ï¼‰
                        frame_point_indices = []
                        if cluster_idx < len(cluster_indices):
                            frame_point_indices = cluster_indices[cluster_idx]
                        
                        if len(frame_object_points) > 0:
                            object_points.append(frame_object_points)
                            object_frames.append(frame_idx)
                            object_indices.append(frame_point_indices)
                
                if object_points:
                    # ç®€å•èšåˆï¼šä½¿ç”¨ä¸­é—´å¸§çš„ç‚¹äº‘
                    middle_idx = len(object_points) // 2
                    aggregated_points = object_points[middle_idx]
                    
                    # å°è¯•ä»VGGTé¢„æµ‹ä¸­æå–gaussianå‚æ•°
                    if preds is not None:
                        # è¿™é‡Œæ²¡æœ‰aggregated_colorsï¼Œå› ä¸º_create_objects_from_clustering_results
                        # å¤„ç†çš„æ˜¯åŸå§‹èšç±»ç»“æœï¼Œä¸æ˜¯å…‰æµèšåˆç»“æœ
                        aggregated_gaussian = self._extract_gaussian_params_from_preds(
                            aggregated_points, preds, None
                        )
                        if aggregated_gaussian is None:
                            # å›é€€æ–¹æ¡ˆ
                            aggregated_gaussian = self._points_to_gaussian_params_fallback(aggregated_points, global_id)
                    elif gaussian_params is not None and H is not None and W is not None:
                        middle_frame_idx = object_frames[middle_idx]
                        middle_point_indices = object_indices[middle_idx]
                        aggregated_gaussian = self._extract_gaussian_params_from_vggt(
                            middle_point_indices, middle_frame_idx, gaussian_params, H, W
                        )
                        if aggregated_gaussian is None:
                            # å›é€€æ–¹æ¡ˆ
                            aggregated_gaussian = self._points_to_gaussian_params_fallback(aggregated_points, global_id)
                    else:
                        # å›é€€æ–¹æ¡ˆ
                        aggregated_gaussian = self._points_to_gaussian_params_fallback(aggregated_points, global_id)
                    
                    # ä¸ºStage2Refineråˆ›å»ºæ¯å¸§çš„Gaussianå‚æ•°å’Œåˆå§‹å˜æ¢
                    frame_gaussians = []
                    initial_transforms = []
                    for i, (frame_points, frame_idx, point_indices) in enumerate(zip(object_points, object_frames, object_indices)):
                        # å°è¯•ä»VGGTæå–è¯¥å¸§çš„gaussianå‚æ•°
                        if preds is not None:
                            frame_gaussian = self._extract_gaussian_params_from_preds(
                                frame_points, preds, None
                            )
                            if frame_gaussian is None:
                                frame_gaussian = self._points_to_gaussian_params_fallback(frame_points, global_id)
                        elif gaussian_params is not None and H is not None and W is not None:
                            frame_gaussian = self._extract_gaussian_params_from_vggt(
                                point_indices, frame_idx, gaussian_params, H, W
                            )
                            if frame_gaussian is None:
                                frame_gaussian = self._points_to_gaussian_params_fallback(frame_points, global_id)
                        else:
                            frame_gaussian = self._points_to_gaussian_params_fallback(frame_points, global_id)
                        
                        frame_gaussians.append(frame_gaussian if frame_gaussian is not None else aggregated_gaussian)
                        # åˆ›å»ºå•ä½å˜æ¢çŸ©é˜µä½œä¸ºåˆå§‹å˜æ¢
                        transform = torch.eye(4, device=self.device)
                        initial_transforms.append(transform)
                    
                    dynamic_objects.append({
                        'object_id': global_id,
                        'aggregated_points': aggregated_points,
                        'aggregated_gaussians': aggregated_gaussian,  # Stage2Refineréœ€è¦çš„å­—æ®µ
                        'frame_gaussians': frame_gaussians,  # Stage2Refineréœ€è¦çš„å­—æ®µ
                        'initial_transforms': initial_transforms,  # Stage2Refineréœ€è¦çš„å­—æ®µ
                        'reference_frame': object_frames[middle_idx],
                        'gaussian_params': aggregated_gaussian,  # ä¿ç•™åŸå­—æ®µä»¥å…¼å®¹
                        'num_frames': len(object_frames)
                    })
            
            return dynamic_objects
            
        except Exception as e:
            return []
    
    def _extract_gaussian_params_from_vggt(
        self, 
        point_indices: List[int], 
        frame_idx: int,
        vggt_gaussian_params: torch.Tensor,
        H: int, W: int
    ) -> Optional[torch.Tensor]:
        """ä»VGGTé¢„æµ‹çš„gaussian_paramsä¸­æå–å¯¹åº”ç‚¹çš„å‚æ•°"""
        try:
            if not point_indices or len(point_indices) == 0:
                return None
            
            if vggt_gaussian_params is None:
                return None
            
            # vggt_gaussian_paramsåº”è¯¥æ˜¯ [S, H*W, 14] æ ¼å¼ï¼ˆå·²ç»ç»è¿‡activationï¼‰
            if len(vggt_gaussian_params.shape) != 3:
                return None
            
            S, HW, feature_dim = vggt_gaussian_params.shape
            if frame_idx >= S:
                return None
            
            # æå–è¯¥å¸§å¯¹åº”ç‚¹çš„gaussianå‚æ•°
            frame_gaussians = vggt_gaussian_params[frame_idx]  # [H*W, 14]
            
            # æ ¹æ®ç‚¹ç´¢å¼•æå–å¯¹åº”çš„gaussianå‚æ•°
            selected_gaussians = []
            for idx in point_indices:
                if 0 <= idx < HW:
                    selected_gaussians.append(frame_gaussians[idx])
            
            if not selected_gaussians:
                return None
            
            # å †å æˆ [N, 14] å¼ é‡
            gaussian_params = torch.stack(selected_gaussians, dim=0)
            return gaussian_params
            
        except Exception as e:
            return None
    
    def _points_to_gaussian_params(self, aggregated_object, preds=None) -> Optional[torch.Tensor]:
        """å°†èšåˆç‰©ä½“è½¬æ¢ä¸ºGaussianå‚æ•°ï¼Œä½¿ç”¨çœŸå®çš„VGGTé¢„æµ‹å‚æ•°"""
        if aggregated_object is None:
            return None
            
        # ä¼˜å…ˆä»èšåˆç‰©ä½“ä¸­è·å–çœŸå®çš„Gaussianå‚æ•°
        if isinstance(aggregated_object, dict) and 'aggregated_gaussians' in aggregated_object:
            aggregated_gaussians = aggregated_object['aggregated_gaussians']
            if aggregated_gaussians is not None:
                if isinstance(aggregated_gaussians, np.ndarray):
                    return torch.from_numpy(aggregated_gaussians).to(self.device).float()
                elif isinstance(aggregated_gaussians, torch.Tensor):
                    return aggregated_gaussians.to(self.device).float()
        
        # å¦‚æœæœ‰ç‚¹äº‘ä¿¡æ¯ï¼Œå°è¯•ä»VGGTé¢„æµ‹ä¸­æ‰¾åˆ°å¯¹åº”çš„Gaussianå‚æ•°
        points = None
        if isinstance(aggregated_object, dict):
            points = aggregated_object.get('aggregated_points', [])
        else:
            points = aggregated_object
            
        if points is None or len(points) == 0:
            return None
            
        # ä»VGGTé¢„æµ‹ä¸­æå–å¯¹åº”çš„Gaussianå‚æ•°
        if preds is not None and 'gaussian_params' in preds:
            # å°è¯•ä»aggregated_objectè·å–é¢œè‰²ä¿¡æ¯
            aggregated_colors = None
            if isinstance(aggregated_object, dict):
                aggregated_colors = aggregated_object.get('aggregated_colors')
            return self._extract_gaussian_params_from_preds(points, preds, aggregated_colors)
        
        # æœ€åçš„å›é€€æ–¹æ¡ˆ
        # å°è¯•ä»aggregated_objectè·å–object_id
        object_id = None
        if isinstance(aggregated_object, dict):
            object_id = aggregated_object.get('object_id') or aggregated_object.get('global_id')
        return self._points_to_gaussian_params_fallback(points, object_id)
    
    def _points_to_gaussian_params_correct(self, aggregated_object, preds, clustering_results, global_id) -> Optional[torch.Tensor]:
        """æ­£ç¡®çš„æ–¹æ³•ï¼šé€šè¿‡åƒç´ ç´¢å¼•ç›´æ¥å¯¹åº”Gaussianå‚æ•°ï¼Œè€Œä¸æ˜¯ç©ºé—´æœ€è¿‘é‚»åŒ¹é…"""
        try:
            if preds is None or 'gaussian_params' not in preds:
                return None
                
            gaussian_params = preds['gaussian_params']  # [B, S*H*W, 14]
            
            # ä»aggregated_objectä¸­è·å–å‚è€ƒå¸§ä¿¡æ¯
            reference_frame = aggregated_object.get('middle_frame', 0)
            aggregated_points = aggregated_object.get('aggregated_points', [])
            
            if len(aggregated_points) == 0:
                return None
            
            # ä»clustering_resultsä¸­æ‰¾åˆ°å¯¹åº”å‚è€ƒå¸§çš„èšç±»ç»“æœ
            # æ³¨æ„ï¼šclustering_resultsçš„ç´¢å¼•å¯èƒ½ä¸frame_idxä¸åŒ
            reference_clustering = None
            
            print(f"ğŸ” æŸ¥æ‰¾å‚è€ƒå¸§{reference_frame}ï¼Œclustering_resultsæœ‰{len(clustering_results)}ä¸ªç»“æœ")
            
            # æ–¹æ³•1: ç›´æ¥é€šè¿‡frame_idxåŒ¹é…
            for result in clustering_results:
                frame_idx = result.get('frame_idx')
                print(f"  æ£€æŸ¥clustering_resultsä¸­çš„frame_idx: {frame_idx}")
                if frame_idx == reference_frame:
                    reference_clustering = result
                    break
            
            # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡ç´¢å¼•åŒ¹é…ï¼ˆreference_frameå¯èƒ½æ˜¯ç›¸å¯¹ç´¢å¼•ï¼‰
            if reference_clustering is None and 0 <= reference_frame < len(clustering_results):
                print(f"  é€šè¿‡ç´¢å¼•{reference_frame}ç›´æ¥è®¿é—®clustering_results")
                reference_clustering = clustering_results[reference_frame]
            
            if reference_clustering is None:
                print(f"âš ï¸  æœªæ‰¾åˆ°å‚è€ƒå¸§{reference_frame}çš„èšç±»ç»“æœ")
                print(f"  å¯ç”¨çš„frame_idx: {[r.get('frame_idx') for r in clustering_results]}")
                return self._points_to_gaussian_params_fallback(aggregated_points, global_id)
            
            # è·å–è¯¥ç‰©ä½“åœ¨å‚è€ƒå¸§ä¸­çš„åƒç´ ç´¢å¼•
            global_ids = reference_clustering.get('global_ids', [])
            cluster_indices = reference_clustering.get('cluster_indices', [])
            
            # æ‰¾åˆ°å±äºè¯¥global_idçš„åƒç´ ç´¢å¼•
            # cluster_indicesæ˜¯ä¸€ä¸ªlist of listsï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªèšç±»çš„åƒç´ ç´¢å¼•åˆ—è¡¨
            object_pixel_indices = []
            for i, gid in enumerate(global_ids):
                if gid == global_id:
                    # cluster_indices[i] æ˜¯è¯¥èšç±»çš„æ‰€æœ‰åƒç´ ç´¢å¼•åˆ—è¡¨
                    if i < len(cluster_indices):
                        object_pixel_indices.extend(cluster_indices[i])
            
            if len(object_pixel_indices) == 0:
                print(f"âš ï¸  åœ¨å‚è€ƒå¸§{reference_frame}ä¸­æœªæ‰¾åˆ°ç‰©ä½“{global_id}çš„åƒç´ ç´¢å¼•")
                return self._points_to_gaussian_params_fallback(aggregated_points, global_id)
            
            print(f"ğŸ” ç‰©ä½“{global_id}: åœ¨å‚è€ƒå¸§{reference_frame}æ‰¾åˆ°{len(object_pixel_indices)}ä¸ªåƒç´ ç´¢å¼•")
            
            # ç›´æ¥é€šè¿‡åƒç´ ç´¢å¼•æå–å¯¹åº”çš„Gaussianå‚æ•°
            B, N_total, feature_dim = gaussian_params.shape
            print(f"ğŸ” Gaussianå‚æ•°å½¢çŠ¶: B={B}, N_total={N_total}, feature_dim={feature_dim}")
            
            # gaussian_paramsçš„å½¢çŠ¶æ˜¯ [B, S*H*W, 14]ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ­£ç¡®çš„å…¨å±€ç´¢å¼•
            # cluster_indicesä¸­çš„åƒç´ ç´¢å¼•æ˜¯ç›¸å¯¹äºå•å¸§çš„ï¼ˆ0åˆ°H*W-1ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºå…¨å±€ç´¢å¼•
            
            # é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ¨æ–­H, Wå’ŒS
            # ä»clustering_resultsæ¨æ–­å‡ºH*W
            H_W = len(reference_clustering.get('points', []))
            if H_W == 0:
                print(f"âš ï¸  æ— æ³•æ¨æ–­å›¾åƒå°ºå¯¸")
                return self._points_to_gaussian_params_fallback(aggregated_points, global_id)
            
            # ä»N_totalå’ŒH_Wæ¨æ–­S
            S = N_total // H_W if H_W > 0 else 1
            print(f"ğŸ” æ¨æ–­çš„å‚æ•°: H*W={H_W}, S={S}, reference_frame={reference_frame}")
            
            selected_gaussians_list = []
            
            for pixel_idx in object_pixel_indices:
                # è®¡ç®—åœ¨å…¨å±€flattenç»“æ„ä¸­çš„ç´¢å¼•
                # å…¨å±€ç´¢å¼• = reference_frame * H*W + pixel_idx
                global_idx = reference_frame * H_W + pixel_idx
                
                if 0 <= global_idx < N_total:
                    selected_gaussians_list.append(gaussian_params[0, global_idx])  # ä½¿ç”¨batch=0
                    print(f"  æå–åƒç´ {pixel_idx}->å…¨å±€ç´¢å¼•{global_idx}çš„Gaussianå‚æ•°")
                else:
                    print(f"  âš ï¸  å…¨å±€ç´¢å¼•{global_idx}(æ¥è‡ªåƒç´ {pixel_idx})è¶…å‡ºèŒƒå›´[0, {N_total-1}]")
            
            if len(selected_gaussians_list) == 0:
                print(f"âš ï¸  æ— æ³•æå–æœ‰æ•ˆçš„Gaussianå‚æ•°")
                return self._points_to_gaussian_params_fallback(aggregated_points, global_id)
            
            selected_gaussians = torch.stack(selected_gaussians_list, dim=0)  # [N, 14]
            
            # æ¿€æ´»Gaussianå‚æ•°
            selected_gaussians = self._apply_gaussian_activation(selected_gaussians)
            
            # ä½¿ç”¨èšåˆåçš„ç‚¹äº‘ä½ç½®æ›¿æ¢Gaussiançš„ä½ç½®å‚æ•°
            points_tensor = torch.from_numpy(aggregated_points).to(self.device).float()
            
            # å¦‚æœç‚¹æ•°ä¸åŒ¹é…ï¼Œå–è¾ƒå°çš„æ•°é‡
            min_count = min(len(selected_gaussians), len(points_tensor))
            selected_gaussians = selected_gaussians[:min_count]
            points_tensor = points_tensor[:min_count]
            
            selected_gaussians[:, :3] = points_tensor[:, :3]
            
            print(f"âœ… æ­£ç¡®æå–äº†{len(selected_gaussians)}ä¸ªGaussianå‚æ•°ï¼ˆé€šè¿‡åƒç´ ç´¢å¼•å¯¹åº”ï¼‰")
            
            return selected_gaussians
            
        except Exception as e:
            print(f"âŒ åƒç´ ç´¢å¼•å¯¹åº”æ–¹æ³•å¤±è´¥: {e}")
            print(f"å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            return self._points_to_gaussian_params_fallback(aggregated_object.get('aggregated_points', []), global_id)
    
    def _extract_gaussian_params_from_preds(self, points, preds, aggregated_colors=None) -> Optional[torch.Tensor]:
        """ä»VGGTé¢„æµ‹ä¸­æå–å¯¹åº”ç‚¹äº‘çš„çœŸå®Gaussianå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨èšåˆé¢œè‰²"""
        try:
            if 'gaussian_params' not in preds or points is None or len(points) == 0:
                return None
            
            gaussian_params = preds['gaussian_params']  # [B, S*H*W, 14]
            
            # ç¡®ä¿pointsæ˜¯torch.Tensor
            if isinstance(points, np.ndarray):
                points = torch.from_numpy(points).to(self.device).float()
            elif isinstance(points, list):
                points = torch.tensor(points, device=self.device, dtype=torch.float32)
            else:
                points = points.to(self.device).float()
            
            # gaussian_paramsçš„shape: [B, S*H*W, 14]
            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸pointsæœ€åŒ¹é…çš„Gaussianå‚æ•°
            B, N_total, feature_dim = gaussian_params.shape
            
            # é‡å¡‘gaussian_paramsä¸º[B*S*H*W, 14]ä»¥ä¾¿å¤„ç†
            gaussian_params_flat = gaussian_params.view(-1, feature_dim)  # [B*S*H*W, 14]
            
            # è·å–Gaussiançš„ä½ç½®ä¿¡æ¯ï¼ˆå‰3ç»´ï¼‰
            gaussian_positions = gaussian_params_flat[:, :3]  # [B*S*H*W, 3]
            
            # ä½¿ç”¨KD-treeæˆ–æœ€è¿‘é‚»æœç´¢æ‰¾åˆ°å¯¹åº”çš„Gaussianå‚æ•°
            from sklearn.neighbors import NearestNeighbors
            
            # å°†ä½ç½®æ•°æ®è½¬æ¢ä¸ºnumpyè¿›è¡ŒKD-treeæœç´¢
            gaussian_pos_np = gaussian_positions.detach().cpu().numpy()
            points_np = points[:, :3].detach().cpu().numpy()
            
            # ä¿®å¤å¤§ç™½çƒé—®é¢˜ï¼šé¿å…é€‰æ‹©ç›¸åŒçš„Gaussianå‚æ•°
            N_points = len(points_np)
            N_gaussians = len(gaussian_pos_np)
            
            if N_gaussians < N_points:
                # å¦‚æœGaussianæ•°é‡å°‘äºç‚¹æ•°ï¼Œéšæœºé‡‡æ ·é¿å…é‡å¤
                selected_indices = np.random.choice(N_gaussians, N_points, replace=True)
                selected_gaussians = gaussian_params_flat[selected_indices]
                print(f"è­¦å‘Šï¼šGaussianæ•°é‡({N_gaussians}) < ç‚¹æ•°({N_points})ï¼Œä½¿ç”¨éšæœºé‡‡æ ·")
            else:
                # ä½¿ç”¨KD-treeä½†ç¡®ä¿æ¯ä¸ªç‚¹éƒ½æœ‰ç‹¬ç‰¹çš„å‚æ•°
                nbrs = NearestNeighbors(n_neighbors=min(5, N_gaussians), algorithm='kd_tree').fit(gaussian_pos_np)
                distances, indices = nbrs.kneighbors(points_np)
                
                # ä¸ºæ¯ä¸ªç‚¹åˆ†é…ä¸åŒçš„Gaussianå‚æ•°ï¼Œé¿å…é‡å¤
                selected_indices = []
                used_indices = set()
                
                for i in range(N_points):
                    # å¯¹äºæ¯ä¸ªç‚¹ï¼Œä»å®ƒçš„æœ€è¿‘é‚»ä¸­é€‰æ‹©ä¸€ä¸ªæœªä½¿ç”¨çš„
                    candidates = indices[i]  # kä¸ªæœ€è¿‘é‚»çš„ç´¢å¼•
                    
                    # ä¼˜å…ˆé€‰æ‹©æœªä½¿ç”¨çš„ç´¢å¼•
                    selected_idx = None
                    for candidate in candidates:
                        if candidate not in used_indices:
                            selected_idx = candidate
                            break
                    
                    # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½è¢«ä½¿ç”¨äº†ï¼Œé€‰æ‹©è·ç¦»æœ€è¿‘çš„
                    if selected_idx is None:
                        selected_idx = candidates[0]
                    
                    selected_indices.append(selected_idx)
                    used_indices.add(selected_idx)
                
                selected_gaussians = gaussian_params_flat[selected_indices]
            
            # å…³é”®ä¿®å¤ï¼šå¯¹ä»VGGTæå–çš„åŸå§‹å‚æ•°è¿›è¡Œæ¿€æ´»å¤„ç†
            # å› ä¸ºVGGTé¢„æµ‹çš„æ˜¯åŸå§‹æœªæ¿€æ´»çš„å‚æ•°ï¼Œéœ€è¦åº”ç”¨æ¿€æ´»å‡½æ•°
            print("å¯¹æå–çš„VGGTå‚æ•°åº”ç”¨æ¿€æ´»å‡½æ•°...")
            selected_gaussians = self._apply_gaussian_activation(selected_gaussians)
            
            # ä½¿ç”¨èšåˆåçš„ç‚¹äº‘ä½ç½®æ›¿æ¢Gaussiançš„ä½ç½®å‚æ•°ï¼ˆæ¿€æ´»åï¼‰
            selected_gaussians[:, :3] = points[:, :3]
            
            # ä¿æŒVGGTé¢„æµ‹å‚æ•°çš„åŸå§‹æ€§ï¼Œä¸æ·»åŠ éšæœºæ‰°åŠ¨
            # å¤§ç™½çƒé—®é¢˜ä¸»è¦é€šè¿‡ç¡®ä¿é€‰æ‹©ä¸åŒçš„Gaussianå‚æ•°æ¥è§£å†³
            
            # ä¿æŒVGGTé¢„æµ‹çš„é¢œè‰²å‚æ•°ï¼Œä¸ç”¨å…‰æµèšåˆçš„é¢œè‰²æ›¿æ¢
            # VGGTé¢„æµ‹çš„é¢œè‰²å‚æ•°ç»è¿‡ç¥ç»ç½‘ç»œè®­ç»ƒï¼Œé€‚åˆ3D Gaussian Splattingæ¸²æŸ“
            # å…‰æµèšåˆçš„é¢œè‰²é€‚åˆä¼ ç»Ÿç‚¹äº‘ï¼Œä½†ä¸é€‚åˆGaussianæ¸²æŸ“
            
            print(f"ä»VGGTé¢„æµ‹ä¸­æå–å¹¶æ¿€æ´»äº† {selected_gaussians.shape[0]} ä¸ªGaussianå‚æ•°")
            
            return selected_gaussians
            
        except Exception as e:
            print(f"ä»VGGTé¢„æµ‹æå–Gaussianå‚æ•°å¤±è´¥: {e}")
            # æ³¨æ„ï¼šä¸è¦å†æ¬¡æ¿€æ´»ï¼Œå› ä¸ºå›é€€æ–¹æ¡ˆä¸­å·²ç»ä¼šæ¿€æ´»
            return self._points_to_gaussian_params_fallback(points, None)
    
    def _points_to_gaussian_params_fallback(self, points, object_id=None) -> Optional[torch.Tensor]:
        """å›é€€æ–¹æ¡ˆï¼šå½“æ— æ³•ä»VGGTæå–æ—¶ï¼Œç”ŸæˆåŸºæœ¬çš„Gaussianå‚æ•°"""
        try:
            if points is None or len(points) == 0:
                return None
            
            # ç¡®ä¿pointsæ˜¯torch.Tensorå¹¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            if isinstance(points, np.ndarray):
                points = torch.from_numpy(points).to(self.device)
            elif isinstance(points, torch.Tensor):
                points = points.to(self.device)
            else:
                return None
            
            N = len(points)
            # åˆ›å»ºåŸºæœ¬çš„Gaussianå‚æ•° [N, 14]: xyz(3) + scale(3) + color(3) + rotation(4) + opacity(1)
            gaussian_params = torch.zeros(N, 14, device=self.device, dtype=torch.float32)
            
            # ä½ç½®: xyz (positions 0:3)
            gaussian_params[:, :3] = points[:, :3]
            
            # å°ºåº¦: scale (positions 3:6) - raw values before activation
            gaussian_params[:, 3:6] = torch.log(torch.tensor(0.01 / 0.05))  # Will become 0.01 after activation
            
            # é¢œè‰²: color (positions 6:9) - ä½¿ç”¨åŸºäºobject_idçš„ä¸€è‡´é¢œè‰²
            if object_id is not None:
                # åŸºäºobject_idç”Ÿæˆä¸€è‡´çš„é¢œè‰²
                import math
                hue = (object_id * 137.5) % 360  # é»„é‡‘è§’åº¦åˆ†å‰²ï¼Œç¡®ä¿é¢œè‰²å·®å¼‚æ˜æ˜¾
                saturation = 0.8
                value = 0.9
                
                # HSVåˆ°RGBè½¬æ¢
                h = hue / 60.0
                c = value * saturation
                x = c * (1 - abs((h % 2) - 1))
                m = value - c
                
                if 0 <= h < 1:
                    r, g, b = c, x, 0
                elif 1 <= h < 2:
                    r, g, b = x, c, 0
                elif 2 <= h < 3:
                    r, g, b = 0, c, x
                elif 3 <= h < 4:
                    r, g, b = 0, x, c
                elif 4 <= h < 5:
                    r, g, b = x, 0, c
                else:
                    r, g, b = c, 0, x
                
                color = torch.tensor([r + m, g + m, b + m], device=self.device)
                gaussian_params[:, 6:9] = color.unsqueeze(0).repeat(N, 1)
                print(f"å›é€€æ–¹æ¡ˆï¼šä¸ºobject_id={object_id}ç”Ÿæˆä¸€è‡´é¢œè‰² RGB=({r+m:.3f}, {g+m:.3f}, {b+m:.3f})")
            else:
                # é»˜è®¤ä¸­æ€§é¢œè‰²
                gaussian_params[:, 6:9] = 0.5
                print("å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨é»˜è®¤ä¸­æ€§é¢œè‰²")
            
            # æ—‹è½¬: quaternion (positions 9:13) - normalized quaternion
            gaussian_params[:, 9:13] = torch.tensor([1.0, 0.0, 0.0, 0.0], device=points.device)  # w, x, y, z
            
            # ä¸é€æ˜åº¦: opacity (position 13) - raw value before sigmoid
            gaussian_params[:, 13] = torch.logit(torch.tensor(0.8, device=points.device))  # Will become 0.8 after sigmoid
            
            # Apply activation functions to get final parameters
            gaussian_params = self._apply_gaussian_activation(gaussian_params)
            
            return gaussian_params
            
        except Exception as e:
            return None
    
    def _apply_gaussian_activation(self, gaussian_params: torch.Tensor) -> torch.Tensor:
        """
        Apply activation functions to gaussian parameters
        Following the same post-processing as in src/vggt/training/loss.py self_render_and_loss
        
        Args:
            gaussian_params: [*, 14] tensor with raw gaussian parameters
            
        Returns:
            gaussian_params: [*, 14] tensor with activated parameters
        """
        if gaussian_params is None:
            return None
            
        
        # Clone to avoid modifying original
        processed_params = gaussian_params.clone()
        
        # Scale activation: (0.05 * exp(scale)).clamp_max(0.3) - applied to positions 3:6
        scale_raw = processed_params[..., 3:6]
        scale_activated = (0.05 * torch.exp(scale_raw)).clamp_max(0.3)
        processed_params[..., 3:6] = scale_activated
        
        # Color: positions 6:9 - no activation needed (kept as is)
        
        # Rotation quaternion normalization: positions 9:13
        rotations = processed_params[..., 9:13]
        rotation_norms = torch.norm(rotations, dim=-1, keepdim=True)
        rotation_norms = torch.clamp(rotation_norms, min=1e-8)
        processed_params[..., 9:13] = rotations / rotation_norms
        
        # Opacity activation: sigmoid for position 13
        opacity_raw = processed_params[..., 13:14]
        opacities = torch.sigmoid(opacity_raw)
        
        # æ¸©å’Œçš„ä¸é€æ˜åº¦å¢å¼ºï¼šé¿å…è¿‡åº¦å‡è´¨åŒ–
        mean_opacity = opacities.mean()
        min_opacity_threshold = 0.2  # æœ€å°ä¸é€æ˜åº¦é˜ˆå€¼
        
        # åªå¢å¼ºè¿‡ä½çš„ä¸é€æ˜åº¦å€¼ï¼Œä¿æŒé«˜çš„å€¼ä¸å˜
        mask_low = opacities < min_opacity_threshold
        if mask_low.any():
            # åªæå‡è¿‡ä½çš„ä¸é€æ˜åº¦åˆ°é˜ˆå€¼ï¼Œä¸å½±å“å…¶ä»–å€¼
            opacities[mask_low] = min_opacity_threshold
            print(f"ä¸é€æ˜åº¦ä¿®æ­£ï¼šæå‡äº† {mask_low.sum().item()} ä¸ªè¿‡ä½çš„ä¸é€æ˜åº¦å€¼åˆ° {min_opacity_threshold}")
        
        # å¦‚æœæ•´ä½“å¹³å‡å€¼ä»ç„¶è¿‡ä½ï¼Œè¿›è¡Œæ¸©å’Œçš„æ•´ä½“æå‡
        if mean_opacity < 0.4:
            # ä½¿ç”¨å¹‚å‡½æ•°è¿›è¡Œæ¸©å’Œæå‡ï¼Œä¿æŒç›¸å¯¹å·®å¼‚
            enhanced_opacities = torch.pow(opacities, 0.7)  # å¹‚ < 1 ä¼šæå‡ä½å€¼ï¼Œä¿æŒé«˜å€¼
            print(f"æ•´ä½“ä¸é€æ˜åº¦æ¸©å’Œæå‡ï¼š{mean_opacity.item():.3f} -> {enhanced_opacities.mean().item():.3f}")
            opacities = enhanced_opacities
        
        processed_params[..., 13:14] = opacities
        
        return processed_params
            
    
    def _validate_and_fix_transform(self, transform: torch.Tensor, frame_idx: int, global_id: int = None) -> bool:
        """éªŒè¯å’Œä¿®å¤å˜æ¢çŸ©é˜µï¼Œé˜²æ­¢å¤§ç™½çƒé—®é¢˜"""
        try:
            # æ£€æŸ¥åŸºæœ¬å½¢çŠ¶
            if transform.shape != (4, 4):
                print(f"âš ï¸  å˜æ¢çŸ©é˜µå½¢çŠ¶å¼‚å¸¸: {transform.shape}, æœŸæœ› (4,4)")
                return False
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé›¶çŸ©é˜µ
            if torch.allclose(transform, torch.zeros_like(transform), atol=1e-8):
                print(f"âš ï¸  å¯¹è±¡{global_id}å¸§{frame_idx}: æ£€æµ‹åˆ°é›¶å˜æ¢çŸ©é˜µï¼è¿™ä¼šå¯¼è‡´å¤§ç™½çƒé—®é¢˜")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
            if torch.isnan(transform).any() or torch.isinf(transform).any():
                print(f"âš ï¸  å¯¹è±¡{global_id}å¸§{frame_idx}: å˜æ¢çŸ©é˜µåŒ…å«NaNæˆ–Infå€¼")
                return False
            
            # æ£€æŸ¥æ—‹è½¬éƒ¨åˆ†çš„è¡Œåˆ—å¼
            rotation_part = transform[:3, :3]
            det = torch.det(rotation_part)
            
            if det.abs() < 1e-6:
                print(f"âš ï¸  å¯¹è±¡{global_id}å¸§{frame_idx}: å˜æ¢çŸ©é˜µå¥‡å¼‚ (det={det:.2e})")
                return False
            
            # æ£€æŸ¥æ˜¯å¦è¿‡åº¦ç¼©æ”¾
            scales = torch.linalg.norm(rotation_part, dim=0)  # å„è½´çš„ç¼©æ”¾
            if scales.max() > 100 or scales.min() < 0.01:
                print(f"âš ï¸  å¯¹è±¡{global_id}å¸§{frame_idx}: å¼‚å¸¸ç¼©æ”¾ {scales}, å¯èƒ½å¯¼è‡´æ¸²æŸ“é—®é¢˜")
                return False
            
            # æ£€æŸ¥å¹³ç§»æ˜¯å¦è¿‡å¤§
            translation = transform[:3, 3]
            if torch.norm(translation) > 1000:
                print(f"âš ï¸  å¯¹è±¡{global_id}å¸§{frame_idx}: å¹³ç§»è¿‡å¤§ {translation}, å¯èƒ½è¶…å‡ºç›¸æœºè§†é‡")
                # è¿™ç§æƒ…å†µä»ç„¶ä¿ç•™ï¼Œä½†ç»™å‡ºè­¦å‘Š
            
            return True
            
        except Exception as e:
            print(f"âŒ å˜æ¢çŸ©é˜µéªŒè¯å¤±è´¥: {e}")
            return False
    
    def _create_static_background(
        self, 
        preds: Dict[str, Any], 
        velocity: Optional[torch.Tensor],
        clustering_results: List[Dict],
        H: int, W: int, S: int
    ) -> torch.Tensor:
        """ä»VGGTé¢„æµ‹çš„Gaussiansä¸­åˆ†ç¦»é™æ€èƒŒæ™¯"""
        try:
            import time
            stage5_times = {}
            
            # Step 1: è·å–å’Œæ•´å½¢Gaussianå‚æ•°
            step1_start = time.time()
            gaussian_params = preds.get('gaussian_params')  # [B, S*H*W, 14] or [B*S, H*W, 14]
            
            if gaussian_params is None:
                return self._create_default_static_background(H, W)
            
            # é‡æ–°æ•´å½¢Gaussianå‚æ•°ä¸º [S, H*W, 14]ï¼ˆæ¿€æ´»å‡½æ•°å·²åœ¨å‡½æ•°å¼€å¤´åº”ç”¨ï¼‰
            if gaussian_params.dim() == 3 and gaussian_params.shape[1] == S * H * W:
                # æƒ…å†µ1: [B, S*H*W, 14] -> [S, H*W, 14]
                gaussian_params = gaussian_params[0].reshape(S, H * W, 14)
            elif gaussian_params.dim() == 3 and gaussian_params.shape[0] == S:
                # æƒ…å†µ2: [S, H*W, 14] -> å·²ç»æ˜¯æ­£ç¡®å½¢çŠ¶
                gaussian_params = gaussian_params
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•é‡æ–°æ•´å½¢
                gaussian_params = gaussian_params.reshape(S, H * W, 14)
            stage5_times['Step 1: è·å–å’Œæ•´å½¢Gaussianå‚æ•°'] = time.time() - step1_start
            
            # Step 2: å¤„ç†é€Ÿåº¦ä¿¡æ¯
            step2_start = time.time()
            if velocity is not None:
                if len(velocity.shape) == 5:
                    B_v, S_v, H_v, W_v, _ = velocity.shape
                    velocity_reshaped = velocity[0].reshape(S, H * W, 3)  # [S, H*W, 3]
                elif len(velocity.shape) == 4:
                    B_v, S_v, HW_v, _ = velocity.shape
                    velocity_reshaped = velocity[0]  # [S, H*W, 3]
                else:
                    velocity_reshaped = velocity[0].reshape(S, H * W, 3) if velocity.numel() >= S * H * W * 3 else torch.zeros(S, H * W, 3, device=self.device)
                
                # è®¡ç®—é€Ÿåº¦å¤§å°
                velocity_magnitude = torch.norm(velocity_reshaped, dim=-1)  # [S, H*W]
            else:
                velocity_magnitude = torch.zeros(S, H * W, device=self.device)
            stage5_times['Step 2: å¤„ç†é€Ÿåº¦ä¿¡æ¯'] = time.time() - step2_start
            
            # Step 3: æ”¶é›†åŠ¨æ€åŒºåŸŸæ©ç 
            step3_start = time.time()
            dynamic_mask_all = torch.zeros(S, H * W, dtype=torch.bool, device=self.device)
            
            for frame_idx, result in enumerate(clustering_results):
                if frame_idx >= S:
                    break
                dynamic_mask = result.get('dynamic_mask')
                if dynamic_mask is not None and len(dynamic_mask) == H * W:
                    dynamic_mask_all[frame_idx] = dynamic_mask
            stage5_times['Step 3: æ”¶é›†åŠ¨æ€åŒºåŸŸæ©ç '] = time.time() - step3_start
            
            # Step 4: è®¡ç®—é™æ€åŒºåŸŸæ©ç 
            step4_start = time.time()
            velocity_threshold = 0.01  # é€Ÿåº¦é˜ˆå€¼
            static_velocity_mask = velocity_magnitude <= velocity_threshold  # ä½é€Ÿåº¦åŒºåŸŸ
            static_object_mask = ~dynamic_mask_all  # éåŠ¨æ€ç‰©ä½“åŒºåŸŸ
            
            # é™æ€åŒºåŸŸ = ä½é€Ÿåº¦ AND éåŠ¨æ€ç‰©ä½“
            static_mask = static_velocity_mask & static_object_mask  # [S, H*W]
            stage5_times['Step 4: è®¡ç®—é™æ€åŒºåŸŸæ©ç '] = time.time() - step4_start
            
            # Step 5: æ”¶é›†æ‰€æœ‰é™æ€Gaussians
            step5_start = time.time()
            all_static_gaussians = []
            for frame_idx in range(S):
                frame_static_mask = static_mask[frame_idx]
                if frame_static_mask.any():
                    frame_static_gaussians = gaussian_params[frame_idx][frame_static_mask]  # [N_static, 14]
                    all_static_gaussians.append(frame_static_gaussians)
            
            if not all_static_gaussians:
                return self._create_default_static_background(H, W)
            
            # åˆå¹¶æ‰€æœ‰é™æ€Gaussians
            all_static_gaussians = torch.cat(all_static_gaussians, dim=0)  # [Total_N, 14]
            stage5_times['Step 5: æ”¶é›†æ‰€æœ‰é™æ€Gaussians'] = time.time() - step5_start
            
            # Step 6: ä¸‹é‡‡æ ·å’Œå»é‡å¤„ç†
            step6_start = time.time()
            downsampled_static_gaussians = self._downsample_static_gaussians(
                all_static_gaussians, max_points=200000, spatial_threshold=0.01
            )
            stage5_times['Step 6: ä¸‹é‡‡æ ·å’Œå»é‡å¤„ç†'] = time.time() - step6_start
            
            # æ˜¾ç¤ºStage 5å„æ­¥éª¤çš„è¯¦ç»†è€—æ—¶
            print("    Stage 5è¯¦ç»†è€—æ—¶:")
            for step_name, step_time in stage5_times.items():
                print(f"      {step_name}: {step_time:.4f}s")
            
            return downsampled_static_gaussians
            
        except Exception as e:
            return self._create_default_static_background(H, W)
    
    def _create_default_static_background(self, H: int, W: int) -> torch.Tensor:
        """åˆ›å»ºé»˜è®¤é™æ€èƒŒæ™¯ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        try:
            num_background_points = min(1000, H * W // 100)
            background_gaussians = torch.zeros(num_background_points, 14, device=self.device)
            
            # éšæœºåˆ†å¸ƒåœ¨3Dç©ºé—´ä¸­
            background_gaussians[:, :3] = torch.randn(num_background_points, 3, device=self.device) * 2.0
            
            # æ—‹è½¬ï¼ˆå•ä½å››å…ƒæ•°ï¼‰
            background_gaussians[:, 3:7] = torch.tensor([1.0, 0.0, 0.0, 0.0], device=self.device)
            
            # å°ºåº¦
            background_gaussians[:, 7:10] = 0.1
            
            # ä¸é€æ˜åº¦
            background_gaussians[:, 10] = 0.1
            
            # é¢œè‰²ï¼ˆç°è‰²ï¼‰
            background_gaussians[:, 11:14] = 0.3
            
            return background_gaussians
            
        except Exception as e:
            return torch.zeros(100, 14, device=self.device)
    
    def _downsample_static_gaussians(
        self, 
        static_gaussians: torch.Tensor, 
        max_points: int = 2000,
        spatial_threshold: float = 0.05
    ) -> torch.Tensor:
        """å¯¹é™æ€Gaussiansè¿›è¡Œä¸‹é‡‡æ ·å’Œå»é‡"""
        try:
            import time
            downsample_times = {}
            print(f"        å¼€å§‹ä¸‹é‡‡æ ·: è¾“å…¥ç‚¹æ•°={len(static_gaussians)}, max_points={max_points}, spatial_threshold={spatial_threshold}")
            if len(static_gaussians) <= max_points:
                print(f"        è·³è¿‡ä¸‹é‡‡æ ·: ç‚¹æ•°å·²ç¬¦åˆè¦æ±‚")
                return static_gaussians
            
            # Step 6.1: åŸºäºç©ºé—´è·ç¦»çš„å»é‡
            step61_start = time.time()
            positions = static_gaussians[:, :3]  # [N, 3]
            
            # ä½¿ç”¨ç®€å•çš„ç½‘æ ¼ä¸‹é‡‡æ ·æ¥å»é‡
            # å°†3Dç©ºé—´åˆ†ä¸ºç½‘æ ¼ï¼Œæ¯ä¸ªç½‘æ ¼åªä¿ç•™ä¸€ä¸ªä»£è¡¨ç‚¹
            grid_size = spatial_threshold
            
            # é‡åŒ–ä½ç½®åˆ°ç½‘æ ¼
            quantized_positions = torch.round(positions / grid_size) * grid_size
            downsample_times['Step 6.1: ä½ç½®é‡åŒ–'] = time.time() - step61_start
            
            # Step 6.2: æ‰¾åˆ°å”¯ä¸€çš„ç½‘æ ¼ä½ç½®
            unique_positions, inverse_indices = torch.unique(
                quantized_positions, dim=0, return_inverse=True
            )
            downsample_times['Step 6.2: å”¯ä¸€ç½‘æ ¼è®¡ç®—'] = time.time() - step62_start
            print(f"        å»é‡å‰: {len(static_gaussians)} -> å»é‡å: {len(unique_positions)}")
            
            # Step 6.3: é€‰æ‹©ä»£è¡¨Gaussian (å‘é‡åŒ–ä¼˜åŒ–)
            step63_start = time.time()
            # ä½¿ç”¨sortæ‰¾åˆ°æ¯ä¸ªç½‘æ ¼çš„ç¬¬ä¸€ä¸ªå‡ºç°ä½ç½®
            sorted_indices, sort_order = torch.sort(inverse_indices)
            # æ‰¾åˆ°ç›¸é‚»å…ƒç´ ä¸åŒçš„ä½ç½®ï¼ˆå³æ¯ä¸ªå”¯ä¸€å€¼ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ï¼‰
            first_occurrence_mask = torch.cat([
                torch.tensor([True], device=self.device), 
                sorted_indices[1:] != sorted_indices[:-1]
            ])
            first_occurrence = sort_order[first_occurrence_mask]
            deduped_gaussians = static_gaussians[first_occurrence]
            downsample_times['Step 6.3: é€‰æ‹©ä»£è¡¨Gaussian'] = time.time() - step63_start
            
            # Step 6.4: éšæœºä¸‹é‡‡æ ·ï¼ˆå¦‚æœä»ç„¶å¤ªå¤šï¼‰
            step64_start = time.time()
            if len(deduped_gaussians) > max_points:
                indices = torch.randperm(len(deduped_gaussians), device=self.device)[:max_points]
                final_gaussians = deduped_gaussians[indices]
                print(f"        éšæœºä¸‹é‡‡æ ·: {len(deduped_gaussians)} -> {len(final_gaussians)}")
            else:
                final_gaussians = deduped_gaussians
                print(f"        æœ€ç»ˆç‚¹æ•°: {len(final_gaussians)}")
            downsample_times['Step 6.4: éšæœºä¸‹é‡‡æ ·'] = time.time() - step64_start
            
            # æ˜¾ç¤ºä¸‹é‡‡æ ·è¯¦ç»†è€—æ—¶
            print("        ä¸‹é‡‡æ ·è¯¦ç»†è€—æ—¶:")
            for step_name, step_time in downsample_times.items():
                print(f"          {step_name}: {step_time:.4f}s")
            
            return final_gaussians
            
        except Exception as e:
            # å›é€€åˆ°ç®€å•éšæœºé‡‡æ ·
            if len(static_gaussians) > max_points:
                indices = torch.randperm(len(static_gaussians), device=self.device)[:max_points]
                return static_gaussians[indices]
            else:
                return static_gaussians
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        total_time = self.processing_stats['total_processing_time']
        total_sequences = max(self.processing_stats['total_sequences'], 1)
        
        return {
            'total_sequences_processed': self.processing_stats['total_sequences'],
            'total_objects_detected': self.processing_stats['total_objects_detected'],
            'avg_objects_per_sequence': self.processing_stats['total_objects_detected'] / total_sequences,
            'avg_processing_time': total_time / total_sequences,
            'sam_time_ratio': self.processing_stats['sam_time'] / max(total_time, 1e-6),
            'optical_flow_time_ratio': self.processing_stats['optical_flow_time'] / max(total_time, 1e-6),
            'aggregation_time_ratio': self.processing_stats['aggregation_time'] / max(total_time, 1e-6)
        }
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if self.temporal_cache:
            self.temporal_cache.clear()
        
        # æ¸…ç†GPUå†…å­˜
        if self.memory_efficient:
            torch.cuda.empty_cache()