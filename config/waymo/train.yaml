# Stage 1 Online Training Configuration for VGGT

debug: true

# =============== Model Configuration ===============
model: "VGGT(img_size=518, patch_size=14, embed_dim=1024, sh_degree=0, use_gs_head=True, use_gs_head_velocity=True, use_gs_head_segment=False, use_gt_camera=True, velocity_head_small_init=False)"
# sh_degree: Spherical harmonics degree (0=DC only 3 params, 1=DC+directional 12 params, 2=27 params, 3=48 params)
# use_gs_head: Use DPTGSHead for gaussian_head (True=DPTGSHead, False=DPTHead)
#   - True: gaussian_head uses DPTGSHead (default)
#   - False: gaussian_head uses standard DPTHead
# use_gs_head_velocity: Use DPTGSHead for velocity_head (True=DPTGSHead, False=DPTHead)
#   - True: velocity_head uses DPTGSHead
#   - False: velocity_head uses standard DPTHead (default)
# use_gs_head_segment: Use DPTGSHead for segment_head (True=DPTGSHead, False=DPTHead)
#   - True: segment_head uses DPTGSHead
#   - False: segment_head uses standard DPTHead (default)
# Gaussian head output_dim is auto-calculated based on sh_degree: 11 + 3*(sh_degree+1)^2
# use_gt_camera: Use GT camera parameters in model forward for world_points calculation
#   - True: Use GT extrinsics and intrinsics
#   - False: Use model predicted pose_enc (default)
# velocity_head_small_init: Small initialization for velocity_head last layer
#   - True: Last layer weight initialized to small value (std=0.001), bias to 0, velocity starts near 0
#   - False: Use default initialization (default)

# pretrained: model.pt
# pretrained_velocity: False
pretrained_velocity: "src/checkpoints/new/start/checkpoint-epoch_0_19533.pth"

# VGGT freeze strategy configuration
# Options: "none", "noap", null (null means no freeze strategy, keep model default behavior)
vggt_freeze_strategy: "none"


# =============== Auxiliary Model Configuration ===============
auxiliary_models:
  # # DAM2 model (for generating sky masks)
  # dam2: DepthAnythingV2(encoder="vitb", offload=${offload_auxiliary})

offload_auxiliary: False

# =============== Training Configuration ===============
load_only_encoder: False
long_context: True
fixed_length: False
resume: null
benchmark: False
num_views: 4
num_test_views: 16

# =============== Stage 1 Loss Weight Configuration ===============
# Weight of 0 means the loss is not computed

# Self Render Loss (trains gaussian head)
self_render_weight: 0.0                    # Main loss for Gaussian parameter learning
self_render_rgb_weight: 1.0                # RGB loss weight
self_render_lpips_weight: 0.0              # LPIPS perceptual loss weight
self_render_depth_weight: 0.0              # Depth loss weight

# GT Flow Loss Ours (custom implementation)
gt_flow_loss_ours_weight: 1.0              # Custom GT flowmap loss weight

# Sky Opacity Loss (supervises gaussian head opacity parameter)
sky_opacity_weight: 0.0                    # Sky region opacity constraint

# Velocity Regularization Loss (constrains velocity values)
velocity_reg_weight: 0.0                   # Velocity regularization constraint

# Segmentation Loss (trains segment head)
segment_loss_weight: 1.0                   # Semantic segmentation loss weight (GT labels from flowmap 4th dimension)
                                           # 4 classes: bg(0), vehicle(1), sign(2), pedestrian+cyclist(3)

# Camera Loss (trains camera head)
camera_loss_weight: 5.0                    # Camera parameter prediction loss weight

# Depth Loss (trains depth head)
conf_depth_loss_weight: 1.0                # Depth prediction loss weight
grad_depth_loss_weight: 0.0                # Depth gradient loss weight
reg_depth_loss_weight: 1.0                 # Depth regularization loss weight

# Scale Loss (trains scale head, uses depth_scale_factor as GT)
scale_loss_weight: 1.0                     # Scene scale prediction loss weight

# Aggregator_all Render Loss (replaces Stage2 refine network rendering loss)
# This loss directly uses dynamic object processing + rendering supervision in Stage1, skipping Stage2 refine network
aggregator_all_start_iter: 0               # Iteration to start aggregator_all loss (default 50k)
aggregator_all_render_rgb_weight: 1.0      # Aggregator_all RGB rendering loss weight
aggregator_all_render_depth_weight: 1.0    # Aggregator_all depth rendering loss weight
aggregator_all_render_lpips_weight: 0.1    # Aggregator_all LPIPS rendering loss weight
aggregator_all_detach_velocity: true       # Detach velocity to prevent gradient backprop to velocity head
                                           # true: velocity.detach() prevents gradient backprop, only use velocity to supervise dynamic objects
                                           # false: Allow rendering loss gradient backprop to velocity head (default)
# Stage2 configuration (rendering config for aggregator_all loss)
stage2_render_only_dynamic: false          # Render only dynamic objects
stage2_supervise_only_dynamic: false       # Supervise only dynamic regions

# Rendering configuration (for self_render_loss and aggregator_all_loss)
enable_voxel_pruning: true                 # Enable voxel pruning
voxel_size: 0.002                          # Voxel size (metric scale, meters)

# Dynamic processor configuration (for aggregator_all loss dynamic object processing)
min_object_size: 500                       # Minimum object size (points)
max_objects_per_frame: 10                  # Maximum objects per frame
velocity_threshold: 0.1                    # Velocity threshold (metric scale, m/s)
clustering_eps: 0.3                        # DBSCAN neighborhood radius
clustering_min_samples: 10                 # DBSCAN minimum samples
tracking_position_threshold: 2.0           # Position matching threshold
tracking_velocity_threshold: 0.2           # Velocity matching threshold
enable_optical_flow_aggregation: true      # Use optical flow aggregation
use_velocity_based_transform: true         # Use velocity for direct transform (false=optical flow method, true=velocity method)
velocity_transform_mode: "procrustes"      # Velocity transform mode: "simple"(T only) or "procrustes"(R+T)

# =============== Dataset Configuration ===============

train_dataset: 500000 @ Waymo_Multi(
  ROOT="data/waymo/train_full",
  valid_camera_id_list=["1", "2", "3"],
  intervals=[5],
  resolution=[(518, 336)], transform=ImgNorm,
  num_views=${num_views}, zero_ground_velocity=True,
  multi_camera_mode=True)  # Enable multi-camera mode

test_dataset: 1000 @ Waymo_Multi(
  ROOT="data/waymo/train_full",
  valid_camera_id_list=["1", "2", "3"],
  intervals=[1],
  resolution=[(518, 378),(518, 336),(518, 294),(518, 252),(518, 210),(518, 140),(378, 518),(336, 518),(294, 518),(252, 518)], num_views=${num_test_views}, seed=42, zero_ground_velocity=True,
  multi_camera_mode=True)  # Enable multi-camera mode

# =============== Optimizer Configuration ===============
seed: 0
batch_size: 1
accum_iter: 4
gradient_checkpointing: True
epochs: 1
start_epoch: 0
weight_decay: 0.05
lr: 3e-5
min_lr: 1e-8
warmup_epochs: 0.02
amp: 1

# =============== Training Monitoring Configuration ===============
num_workers: 4
world_size: 1
local-rank: -1
dist_url: 'env://'
rank: 0
gpu: 0
distributed: False
dist_backend: 'nccl'

eval_freq: 1
save_freq: 0.05                             # Save every 10% of epoch
keep_freq: 2
print_freq: 10
print_img_freq: 50000000
num_imgs_vis: 4

# =============== Output Configuration ===============
save_dir: 'checkpoints/new'
exp_name: 'start'
task: 'cut3r'
logdir: ./${save_dir}/${exp_name}/logs
output_dir: ./${save_dir}/${exp_name}/

# =============== Hydra Configuration ===============
hydra:
  verbose: False
  run:
    dir: ./${save_dir}/${exp_name}